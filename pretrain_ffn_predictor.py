#!/usr/bin/env python3
"""
Pre-train a lightweight dynamics predictor that maps the current quadrotor state
(position, velocity, angular velocity, rotation matrix) to the change in position
and rotation observed one simulator step later.

Inputs  : [position(3), velocity(3), angular_velocity(3), rotation(9)]  -> total 18 scalars
Targets : [delta_position(3), delta_rotation(9)]                        -> total 12 scalars

The script consumes datasets generated by collect_dynamics_dataset.py and stores
checkpoints under <train_dir>/<experiment_name>/, keeping both a rolling "latest"
and the best validation-loss model (filenames include the val loss).
"""

import argparse
import glob
import os
import sys
from datetime import datetime
from typing import Dict, Optional, Tuple

import numpy as np
import torch
from torch import nn
from torch.utils.data import DataLoader, Subset, TensorDataset
from tqdm import tqdm


REQUIRED_KEYS = (
    "position",
    "velocity",
    "angular_velocity",
    "rotation",
    "delta_position",
    "delta_rotation",
)


def set_seed(seed: int) -> None:
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def load_dataset(path: str) -> Dict[str, np.ndarray]:
    data = np.load(path)
    missing = [key for key in REQUIRED_KEYS if key not in data]
    if missing:
        raise KeyError(f"Dataset {path} is missing required keys: {missing}")
    return {key: data[key] for key in REQUIRED_KEYS}


def assemble_features_targets(data: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:
    def _reshape(name: str) -> np.ndarray:
        arr = np.asarray(data[name], dtype=np.float32)
        return arr.reshape(arr.shape[0], -1)

    inputs = np.concatenate(
        [
            _reshape("position"),
            _reshape("velocity"),
            _reshape("angular_velocity"),
            _reshape("rotation"),
        ],
        axis=1,
    )
    targets = np.concatenate(
        [
            _reshape("delta_position"),
            _reshape("delta_rotation"),
        ],
        axis=1,
    )

    if inputs.shape[0] != targets.shape[0]:
        raise ValueError(
            f"Input count ({inputs.shape[0]}) does not match target count ({targets.shape[0]})."
        )
    return inputs, targets


def build_model(input_dim: int, output_dim: int, hidden_size: int, num_layers: int) -> nn.Module:
    if num_layers < 1:
        raise ValueError("num_layers must be >= 1")

    layers = []
    in_dim = input_dim
    for _ in range(num_layers):
        layers.append(nn.Linear(in_dim, hidden_size))
        layers.append(nn.ReLU())
        in_dim = hidden_size
    layers.append(nn.Linear(in_dim, output_dim))
    return nn.Sequential(*layers)


def train_loop(
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    *,
    device: torch.device,
    epochs: int,
    optimizer: torch.optim.Optimizer,
    criterion: nn.Module,
    log_fn,
    delta_pos_slice: slice,
    start_epoch: int = 0,
    epoch_callback=None,
) -> Tuple[list, list]:
    model.to(device)
    train_losses, val_losses = [], []

    for epoch in range(1, epochs + 1):
        actual_epoch = start_epoch + epoch
        model.train()
        running_loss = 0.0
        delta_pos_train = 0.0

        for batch_inputs, batch_targets in train_loader:
            batch_inputs = batch_inputs.to(device)
            batch_targets = batch_targets.to(device)

            optimizer.zero_grad(set_to_none=True)
            preds = model(batch_inputs)
            loss = criterion(preds, batch_targets)
            loss.backward()
            optimizer.step()

            batch_size = batch_inputs.size(0)
            running_loss += loss.item() * batch_size

            preds_delta = preds[:, delta_pos_slice]
            target_delta = batch_targets[:, delta_pos_slice]
            delta_pos_loss = torch.mean((preds_delta - target_delta) ** 2)
            delta_pos_train += delta_pos_loss.item() * batch_size

        epoch_train_loss = running_loss / len(train_loader.dataset)
        epoch_delta_train = delta_pos_train / len(train_loader.dataset)
        train_losses.append(epoch_train_loss)

        model.eval()
        val_loss_total = 0.0
        val_delta_total = 0.0
        with torch.no_grad():
            for val_inputs, val_targets in val_loader:
                val_inputs = val_inputs.to(device)
                val_targets = val_targets.to(device)
                preds = model(val_inputs)
                loss = criterion(preds, val_targets)

                batch_size = val_inputs.size(0)
                val_loss_total += loss.item() * batch_size

                preds_delta = preds[:, delta_pos_slice]
                target_delta = val_targets[:, delta_pos_slice]
                delta_pos_loss = torch.mean((preds_delta - target_delta) ** 2)
                val_delta_total += delta_pos_loss.item() * batch_size

        epoch_val_loss = val_loss_total / len(val_loader.dataset)
        epoch_val_delta = val_delta_total / len(val_loader.dataset)
        val_losses.append(epoch_val_loss)

        log_fn(
            " | ".join(
                [
                    f"Epoch {actual_epoch:03d}",
                    f"train_loss={epoch_train_loss:.6f}",
                    f"val_loss={epoch_val_loss:.6f}",
                    f"train_delta_pos_loss={epoch_delta_train:.6f}",
                    f"val_delta_pos_loss={epoch_val_delta:.6f}",
                ]
            )
        )
        if epoch_callback is not None:
            epoch_callback(actual_epoch, epoch_val_loss)

    return train_losses, val_losses


def main():
    parser = argparse.ArgumentParser(description="Pre-train a delta-position/rotation predictor")
    parser.add_argument("--dataset_path", required=True,
                        help="Path to the NPZ dataset produced by collect_dynamics_dataset.py")
    parser.add_argument("--train_dir", required=True,
                        help="Directory that will hold trained predictor checkpoints")
    parser.add_argument("--experiment_name", required=True,
                        help="Subdirectory inside train_dir for this run")
    parser.add_argument("--load_best", action="store_true",
                        help="If resuming and both checkpoints exist, prefer loading the best model")
    parser.add_argument("--hidden_size", type=int, default=256,
                        help="Hidden layer width")
    parser.add_argument("--num_layers", type=int, default=3,
                        help="Number of hidden layers")
    parser.add_argument("--batch_size", type=int, default=1024,
                        help="Mini-batch size")
    parser.add_argument("--epochs", type=int, default=50,
                        help="Number of training epochs to run")
    parser.add_argument("--lr", type=float, default=1e-3,
                        help="Learning rate for Adam optimizer")
    parser.add_argument("--val_fraction", type=float, default=0.1,
                        help="Fraction of samples to reserve for validation (must be > 0)")
    parser.add_argument("--seed", type=int, default=42,
                        help="Random seed for dataset shuffling and initialization")
    parser.add_argument("--device", default="cuda" if torch.cuda.is_available() else "cpu",
                        help="Training device")
    args = parser.parse_args()

    if args.val_fraction <= 0.0 or args.val_fraction >= 1.0:
        raise ValueError("val_fraction must be in (0, 1)")

    set_seed(args.seed)

    data = load_dataset(args.dataset_path)
    inputs, targets = assemble_features_targets(data)

    num_samples = inputs.shape[0]
    if num_samples < 2:
        raise ValueError("Need at least two samples to build a train/validation split.")

    val_size = max(1, int(round(num_samples * args.val_fraction)))
    val_size = min(val_size, num_samples - 1)
    indices = np.random.permutation(num_samples)
    val_indices = indices[:val_size]
    train_indices = indices[val_size:]

    output_dir = os.path.join(args.train_dir, args.experiment_name)
    os.makedirs(output_dir, exist_ok=True)

    run_log_path = os.path.join(output_dir, "training_runs.log")
    log_file = open(run_log_path, "a", encoding="utf-8")

    def log(message: str) -> None:
        tqdm.write(message)
        log_file.write(message + "\n")
        log_file.flush()

    log("")
    log("=" * 80)
    log(f"Run started: {datetime.now().isoformat(timespec='seconds')}")
    log(f"Command: {' '.join(sys.argv)}")
    log(f"Arguments: {vars(args)}")

    try:
        existing_best_path: Optional[str] = None
        existing_latest_path: Optional[str] = None
        existing_best_data: Optional[Dict[str, object]] = None
        latest_checkpoint_data: Optional[Dict[str, object]] = None

        best_ckpts = sorted(glob.glob(os.path.join(output_dir, "checkpoint_best_val*.pt")))
        if best_ckpts:
            existing_best_path = best_ckpts[-1]
            existing_best_data = torch.load(existing_best_path, map_location="cpu")

        latest_ckpts = sorted(glob.glob(os.path.join(output_dir, "checkpoint_latest_val*.pt")))
        if latest_ckpts:
            existing_latest_path = latest_ckpts[-1]
            if existing_latest_path == existing_best_path:
                latest_checkpoint_data = existing_best_data
            else:
                latest_checkpoint_data = torch.load(existing_latest_path, map_location="cpu")

        resume_checkpoint_path: Optional[str] = None
        resume_checkpoint_data: Optional[Dict[str, object]] = None
        if args.load_best and existing_best_data is not None:
            resume_checkpoint_path = existing_best_path
            resume_checkpoint_data = existing_best_data
        elif latest_checkpoint_data is not None:
            resume_checkpoint_path = existing_latest_path
            resume_checkpoint_data = latest_checkpoint_data

        if resume_checkpoint_data is not None:
            saved_input_dim = int(resume_checkpoint_data.get("input_dim", inputs.shape[1]))
            saved_output_dim = int(resume_checkpoint_data.get("output_dim", targets.shape[1]))
            if saved_input_dim != inputs.shape[1] or saved_output_dim != targets.shape[1]:
                raise ValueError(
                    f"Checkpoint shapes (input={saved_input_dim}, output={saved_output_dim}) "
                    f"do not match dataset (input={inputs.shape[1]}, output={targets.shape[1]})."
                )

        inputs_tensor = torch.from_numpy(inputs.astype(np.float32))
        targets_tensor = torch.from_numpy(targets.astype(np.float32))

        dataset = TensorDataset(inputs_tensor, targets_tensor)
        train_subset = Subset(dataset, train_indices.tolist())
        val_subset = Subset(dataset, val_indices.tolist())

        train_loader = DataLoader(train_subset, batch_size=args.batch_size, shuffle=True, drop_last=False)
        val_loader = DataLoader(val_subset, batch_size=args.batch_size, shuffle=False, drop_last=False)

        device = torch.device(args.device)
        model = build_model(inputs.shape[1], targets.shape[1], args.hidden_size, args.num_layers)
        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
        criterion = nn.MSELoss()

        start_epoch = 0
        if resume_checkpoint_data is not None:
            model.load_state_dict(resume_checkpoint_data["model_state_dict"])
            start_epoch = int(resume_checkpoint_data.get("epochs_trained", 0))
            val_loss_prev = resume_checkpoint_data.get("val_loss", None)
            val_loss_str = f"{val_loss_prev:.6f}" if val_loss_prev is not None else "unknown"
            log(
                f"[pretrain_predictor] Resuming from {resume_checkpoint_path} "
                f"(epochs_trained={start_epoch}, val_loss={val_loss_str})"
            )
        else:
            log(f"[pretrain_predictor] Starting new experiment at {output_dir}")

        delta_pos_slice = slice(0, 3)

        def save_checkpoint(path: str, val_loss_value: float, epoch_idx: int) -> None:
            torch.save(
                {
                    "model_state_dict": {k: v.detach().cpu() for k, v in model.state_dict().items()},
                    "input_dim": inputs.shape[1],
                    "output_dim": targets.shape[1],
                    "hidden_size": args.hidden_size,
                    "num_layers": args.num_layers,
                    "epochs_trained": epoch_idx,
                    "learning_rate": args.lr,
                    "batch_size": args.batch_size,
                    "val_fraction": args.val_fraction,
                    "seed": args.seed,
                    "val_loss": float(val_loss_value),
                },
                path,
            )

        best_val_loss = float("inf")
        if existing_best_data is not None:
            best_val_loss = float(existing_best_data.get("val_loss", float("inf")))
        best_checkpoint_path = existing_best_path
        latest_checkpoint_path = existing_latest_path

        def epoch_callback(epoch_idx: int, val_loss_value: float) -> None:
            nonlocal best_val_loss, best_checkpoint_path, latest_checkpoint_path

            latest_name = f"checkpoint_latest_val{val_loss_value:.6f}.pt"
            latest_path = os.path.join(output_dir, latest_name)
            if latest_checkpoint_path and latest_checkpoint_path != latest_path and os.path.exists(latest_checkpoint_path):
                os.remove(latest_checkpoint_path)
            save_checkpoint(latest_path, val_loss_value, epoch_idx)
            latest_checkpoint_path = latest_path

            if val_loss_value < best_val_loss:
                best_name = f"checkpoint_best_val{val_loss_value:.6f}.pt"
                best_path = os.path.join(output_dir, best_name)
                if best_checkpoint_path and best_checkpoint_path != best_path and os.path.exists(best_checkpoint_path):
                    os.remove(best_checkpoint_path)
                save_checkpoint(best_path, val_loss_value, epoch_idx)
                best_checkpoint_path = best_path
                best_val_loss = val_loss_value
                log(f"[pretrain_predictor] New best checkpoint at epoch {epoch_idx} (val_loss={val_loss_value:.6f})")

        log(
            f"[pretrain_predictor] Dataset size: train={len(train_subset)}, "
            f"val={len(val_subset)}, total={num_samples}"
        )

        train_loop(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            device=device,
            epochs=args.epochs,
            optimizer=optimizer,
            criterion=criterion,
            log_fn=log,
            delta_pos_slice=delta_pos_slice,
            start_epoch=start_epoch,
            epoch_callback=epoch_callback,
        )

        if latest_checkpoint_path:
            log(f"[pretrain_predictor] Latest checkpoint: {os.path.abspath(latest_checkpoint_path)}")
        if best_checkpoint_path:
            log(f"[pretrain_predictor] Best checkpoint: {os.path.abspath(best_checkpoint_path)} "
                f"(val_loss={best_val_loss:.6f})")
    finally:
        log_file.close()


if __name__ == "__main__":
    main()
